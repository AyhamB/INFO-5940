{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup GPT-4o model access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "model = AzureOpenAI(\n",
    "    api_key=environ['AZURE_OPENAI_API_KEY'],\n",
    "    api_version=\"2024-08-01-preview\",\n",
    "    azure_endpoint=environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    azure_deployment=environ['AZURE_OPENAI_MODEL_DEPLOYMENT'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.chat.completions.create(\n",
    "            model='gpt-4o',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Hi, my name is Adam, how are you?\",\n",
    "                },\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from PyPDF2 import PdfReader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs_from_folder(folder_path):\n",
    "    documents = []\n",
    "    file_names = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.lower().endswith(\".pdf\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            # Open and read the PDF using PyPDF2\n",
    "            reader = PdfReader(file_path)\n",
    "            text_content = \"\"\n",
    "            for page in reader.pages:\n",
    "                text_content += page.extract_text()\n",
    "            documents.append(text_content)  # Add text of each PDF to the documents list\n",
    "            file_names.append(file_name)    # Add the file name to the list\n",
    "    return documents, file_names\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"/workspace/Design and Construction Standards/Standards - PDF's\"\n",
    "pdf_documents, pdf_filenames = load_pdfs_from_folder(folder_path)\n",
    "\n",
    "# Print the content of the first document\n",
    "print(pdf_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing by printing the second document\n",
    "print(pdf_documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, model: str = \"gpt-4\") -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "# Load documents and filenames\n",
    "folder_path = \"/workspace/Design and Construction Standards/Standards - PDF's\"\n",
    "pdf_documents, pdf_filenames = load_pdfs_from_folder(folder_path)\n",
    "\n",
    "# List to hold (document name, token count) tuples\n",
    "token_counts = []\n",
    "\n",
    "# Calculate token counts and store them with their document names\n",
    "for doc_name, doc_content in zip(pdf_filenames, pdf_documents):\n",
    "    token_count = num_tokens_from_string(doc_content, model=\"gpt-4\")\n",
    "    token_counts.append((doc_name, token_count))  # Store as (document name, token count)\n",
    "\n",
    "# Sort the list of tuples by token count in descending order\n",
    "token_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted token counts\n",
    "for doc_name, count in token_counts:\n",
    "    print(f\"Document '{doc_name}' Token Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of tokens across all documents\n",
    "total_tokens = sum(count for _, count in token_counts)\n",
    "\n",
    "# Compute the average token count\n",
    "average_tokens = total_tokens / len(token_counts)\n",
    "\n",
    "# Calculate the variance\n",
    "variance = sum((count - average_tokens) ** 2 for _, count in token_counts) / len(token_counts)\n",
    "\n",
    "# Calculate the standard deviation\n",
    "std_dev_tokens = variance ** 0.5\n",
    "\n",
    "# Print the average and standard deviation\n",
    "print(f\"Average Token Count: {average_tokens:.2f}, Standard Deviation: {std_dev_tokens:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 10  # Number of documents per batch\n",
    "MAX_WORKERS = 10  # Number of threads for concurrent processing\n",
    "\n",
    "# Function to find relevant content in a document\n",
    "def find_relevant_content(question, doc_name, text_content):\n",
    "    prompt = f\"\"\"\n",
    "    You are given a user's question and the content of a PDF document.\n",
    "\n",
    "    Question: \"{question}\"\n",
    "\n",
    "    PDF Content:\n",
    "    {text_content}\n",
    "\n",
    "    Your task:\n",
    "\n",
    "    - If the PDF content contains information relevant to the user's question, provide a detailed answer, including quotes from the document where appropriate.\n",
    "\n",
    "    - If the PDF content does not contain information relevant to the question, simply reply: \"No relevant information found.\"\n",
    "\n",
    "    Provide your answer below.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = model.chat.completions.create(\n",
    "            model=environ['AZURE_OPENAI_MODEL_DEPLOYMENT'],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        print(f\"Response for {doc_name}:\\n{answer}\\n\")  # For debugging\n",
    "        if \"no relevant information found\" not in answer.lower():\n",
    "            return doc_name, answer  # Return document name and answer if relevant\n",
    "    except Exception as e:\n",
    "        print(f\"Error with LLM API for {doc_name}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Function to process a batch of PDFs\n",
    "def process_pdf_batch(question, pdf_batch, pdf_batch_filenames):\n",
    "    relevant_info = []\n",
    "    for doc_content, doc_name in zip(pdf_batch, pdf_batch_filenames):\n",
    "        print(f\"Processing {doc_name}\")\n",
    "        result = find_relevant_content(question, doc_name, doc_content)\n",
    "        if result:\n",
    "            relevant_info.append(result)\n",
    "    return relevant_info\n",
    "\n",
    "# Main function\n",
    "def main(question, pdf_documents, pdf_filenames):\n",
    "    all_relevant_info = []\n",
    "    BATCH_SIZE = 10  # Adjust as needed\n",
    "\n",
    "    # Process documents in batches\n",
    "    for i in range(0, len(pdf_documents), BATCH_SIZE):\n",
    "        pdf_batch = pdf_documents[i:i + BATCH_SIZE]\n",
    "        pdf_batch_filenames = pdf_filenames[i:i + BATCH_SIZE]\n",
    "        print(f\"\\nProcessing batch {i // BATCH_SIZE + 1}\")\n",
    "\n",
    "        relevant_info = process_pdf_batch(question, pdf_batch, pdf_batch_filenames)\n",
    "        all_relevant_info.extend(relevant_info)\n",
    "\n",
    "    # Output detailed answers from relevant documents\n",
    "    print(\"\\nDetailed answers from relevant documents:\")\n",
    "    for doc_name, answer in all_relevant_info:\n",
    "        print(f\"Document: {doc_name}\\nAnswer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can a restroom be used as a lactation room?\"\n",
    "main(question, pdf_documents, pdf_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is there a required frit pattern on glass?\"\n",
    "main(question, pdf_documents, pdf_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Does my project have to provide a gender-neutral restroom?\"\n",
    "main(question, pdf_documents, pdf_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Does my project have to provide a toilet room?\"\n",
    "main(question, pdf_documents, pdf_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What VFD manufactures does Cornell University allow on projects?\"\n",
    "main(question, pdf_documents, pdf_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the minimum required lighting level for sidewalks on campus?‚Äù\"\n",
    "main(question, pdf_documents, pdf_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What electrical conduit types are required for direct burial on campus?\"\n",
    "main(question, pdf_documents, pdf_filenames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
