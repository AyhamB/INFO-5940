{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from os import environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    "    api_version=\"2023-06-01-preview\",\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load Source Text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"/workspace/data/knowledge_base/fruits_and_veggies.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Split the document</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100\n",
    "chunk_overlap = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk.page_content)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index chunks into a vector db (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=AzureOpenAIEmbeddings(model=\"text-embedding-3-large\"))\n",
    "#vectorstore = Chroma.from_documents(documents=chunks, embedding=AzureOpenAIEmbeddings(model=\"text-embedding-ada-002\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search(\"Amanita phalloides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that providers implement different scores; Chroma here\n",
    "# returns a distance metric that should vary inversely with similarity.\n",
    "vectorstore.similarity_search_with_score(\"Amanita phalloides\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare prompt (Augmentation Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "    \n",
    "    Question: {question} \n",
    "    \n",
    "    Context: {context} \n",
    "    \n",
    "    Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "retrieved_docs = retriever.invoke(\"Amanita phalloides\")\n",
    "\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_docs(retriever.invoke(\"Amanita phalloides\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"tell me about Amanita phalloides\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Aware Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_similarity(text1, text2):\n",
    "    template = \"\"\"\n",
    "    Analyze the contextual relationship between the following two texts:\n",
    "    \n",
    "    Text 1: {text1}\n",
    "    Text 2: {text2}\n",
    "    \n",
    "    Evaluate whether Text 2 completes or extends the context of Text 1, or if they are separate and unrelated. Assign a float score from 0 to 1, where:\n",
    "    \n",
    "    0 = The texts are entirely unrelated and should be split\n",
    "    1 = The texts are strongly connected and belong to the same context\n",
    "    \n",
    "    Consider factors such as:\n",
    "    \n",
    "    Thematic continuity\n",
    "    Logical flow\n",
    "    Shared subject matter\n",
    "    Narrative or argumentative progression\n",
    "    Linguistic cohesion\n",
    "    Provide only a single float value between 0 and 1 as your response, with up to two decimal places. For example: 0.75\n",
    "    \n",
    "    Ensure your answer contains nothing but the float value. Double-check your response before submitting\"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    similarity = chain.invoke({\"text1\": text1, \"text2\": text2})\n",
    "    return float(similarity.replace('.\\n\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_similarity(\"\"\"The Amanita phalloides has a large and imposing epigeous (above ground) fruiting body (basidiocrap).\"\"\",\n",
    "               \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all white.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_metadata(metadatas):\n",
    "    merged_metadata = {}\n",
    "    for metadata in metadatas:\n",
    "        for key, value in metadata.items():\n",
    "            if key in merged_metadata:\n",
    "                merged_metadata[key] += \" \" + value  \n",
    "            else:\n",
    "                merged_metadata[key] = value\n",
    "    return merged_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def context_text_splitter_with_llm(documents, step_size, chunk_size, max_chunk_size):\n",
    "    # Split the text\n",
    "    # Ensure you have RecursiveCharacterTextSplitter defined and available\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=step_size, chunk_overlap=0)\n",
    "\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    step_chunks = [doc.page_content for doc in docs]\n",
    "    step_metadata = [doc.metadata for doc in docs]\n",
    "\n",
    "    merged_chunks = []\n",
    "    merged_metadata_chunks = []\n",
    "\n",
    "    while len(step_chunks) > 0:\n",
    "        if len(''.join(step_chunks)) < chunk_size:\n",
    "            break\n",
    "        \n",
    "        current_chunk = ''.join(step_chunks[:chunk_size//step_size])\n",
    "        current_metadata = merge_metadata(step_metadata[:chunk_size//step_size])\n",
    "        \n",
    "        step_chunks = step_chunks[chunk_size//step_size:]\n",
    "        step_metadata = step_metadata[chunk_size//step_size:]\n",
    "\n",
    "        chunk_appended = False\n",
    "\n",
    "        while len(step_chunks) > 0:\n",
    "            next_step_chunk = step_chunks.pop(0)\n",
    "            next_step_chunk_metadata = step_metadata.pop(0)\n",
    "\n",
    "            similarity_score = llm_similarity(current_chunk, next_step_chunk)\n",
    "\n",
    "            if similarity_score > 0.49 and len(current_chunk) + len(next_step_chunk) <= max_chunk_size:\n",
    "                current_chunk += \" \" + next_step_chunk\n",
    "                current_metadata = merge_metadata([current_metadata, next_step_chunk_metadata])\n",
    "            else:\n",
    "                merged_chunks.append(\" \" + current_chunk)\n",
    "                merged_metadata_chunks.append(current_metadata)\n",
    "                \n",
    "                chunk_appended = True\n",
    "                \n",
    "                step_chunks.insert(0, next_step_chunk)\n",
    "                step_metadata.insert(0, next_step_chunk_metadata)\n",
    "                \n",
    "                break\n",
    "\n",
    "        if not chunk_appended:\n",
    "            merged_chunks.append(\" \" + current_chunk)\n",
    "            merged_metadata_chunks.append(current_metadata)\n",
    "\n",
    "    if len(step_chunks) > 0:\n",
    "        merged_chunks.append(' '.join(step_chunks))\n",
    "        merged_metadata_chunks.append(merge_metadata(step_metadata))\n",
    "\n",
    "    merged_docs = []\n",
    "    for chunk, metadata in zip(merged_chunks, merged_metadata_chunks):\n",
    "        merged_docs.append(Document(page_content=chunk, metadata=metadata))\n",
    "\n",
    "    return merged_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = context_text_splitter_with_llm(documents, 100, 200, 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk.page_content)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index into Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Access a specific collection\n",
    "collection_name = \"langchain\"\n",
    "client.delete_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=AzureOpenAIEmbeddings(model=\"text-embedding-3-large\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"tell me about Amanita phalloides\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Vector Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = embeddings.embed_documents(texts=[\"Apple\"])[0]\n",
    "v2 = embeddings.embed_documents(texts=[\"Orange\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute the cosine similarity between two vectors using SciPy.\"\"\"\n",
    "    return 1 - cosine(vec1, vec2)  # cosine function from SciPy computes the distance, not similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "def context_text_splitter(documents, step_size, chunk_size, max_chunk_size):\n",
    "    # Split the text\n",
    "    # Ensure you have RecursiveCharacterTextSplitter defined and available\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=step_size, chunk_overlap=0)\n",
    "\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    step_chunks = [doc.page_content for doc in docs]\n",
    "    step_metadata = [doc.metadata for doc in docs]\n",
    "\n",
    "    merged_chunks = []\n",
    "    merged_metadata_chunks = []\n",
    "\n",
    "    while len(step_chunks) > 0:\n",
    "        if len(''.join(step_chunks)) < chunk_size:\n",
    "            break\n",
    "        \n",
    "        current_chunk = ''.join(step_chunks[:chunk_size//step_size])\n",
    "        current_metadata = merge_metadata(step_metadata[:chunk_size//step_size])\n",
    "        \n",
    "        step_chunks = step_chunks[chunk_size//step_size:]\n",
    "        step_metadata = step_metadata[chunk_size//step_size:]\n",
    "\n",
    "        chunk_appended = False\n",
    "\n",
    "        while len(step_chunks) > 0:\n",
    "            next_step_chunk = step_chunks.pop(0)\n",
    "            next_step_chunk_metadata = step_metadata.pop(0)\n",
    "\n",
    "            similarity_score = cosine_similarity(embeddings.embed_query(current_chunk), embeddings.embed_query(next_step_chunk))\n",
    "\n",
    "            if similarity_score > 0.79 and len(current_chunk) + len(next_step_chunk) <= max_chunk_size:\n",
    "                current_chunk += \" \" + next_step_chunk\n",
    "                current_metadata = merge_metadata([current_metadata, next_step_chunk_metadata])\n",
    "            else:\n",
    "                merged_chunks.append(\" \" + current_chunk)\n",
    "                merged_metadata_chunks.append(current_metadata)\n",
    "                \n",
    "                chunk_appended = True\n",
    "                \n",
    "                step_chunks.insert(0, next_step_chunk)\n",
    "                step_metadata.insert(0, next_step_chunk_metadata)\n",
    "                \n",
    "                break\n",
    "\n",
    "        if not chunk_appended:\n",
    "            merged_chunks.append(\" \" + current_chunk)\n",
    "            merged_metadata_chunks.append(current_metadata)\n",
    "\n",
    "    if len(step_chunks) > 0:\n",
    "        merged_chunks.append(' '.join(step_chunks))\n",
    "        merged_metadata_chunks.append(merge_metadata(step_metadata))\n",
    "\n",
    "    merged_docs = []\n",
    "    for chunk, metadata in zip(merged_chunks, merged_metadata_chunks):\n",
    "        merged_docs.append(Document(page_content=chunk, metadata=metadata))\n",
    "\n",
    "    return merged_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = context_text_splitter(documents, 100, 200, 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk.page_content)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
