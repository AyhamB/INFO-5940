{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NEO4J_URI\"] = \"bolt://neo4j:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"your_password\"\n",
    "\n",
    "graph = Neo4jGraph(refresh_schema=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, model: str = \"gpt-4o\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = pd.read_csv(\n",
    "    \"/workspace/data/IT-job-data/Position_Descriptions_by_Job_Profile_-_Generative_AI_Experiment1.csv\"\n",
    ")\n",
    "jobs[\"tokens\"] = [\n",
    "    num_tokens_from_string(f\"{row['Job Description Summary']} {row['Job Description']} {row['Job Description Responsibilities']}\")\n",
    "    for i, row in jobs.iterrows()\n",
    "]\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sns.histplot(jobs[\"tokens\"], kde=False)\n",
    "plt.title('Distribution of chunk sizes')\n",
    "plt.xlabel('Token count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from os import environ\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(\n",
    "    llm=llm,\n",
    "    node_properties=[\"description\"],\n",
    "    relationship_properties=[\"description\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List\n",
    "from langchain_community.graphs.graph_document import GraphDocument\n",
    "from langchain_core.documents import Document\n",
    "from retry import retry  # Import the retry decorator\n",
    "import random\n",
    "\n",
    "# Define constants\n",
    "MAX_WORKERS = 10\n",
    "NUM_ARTICLES = 20\n",
    "MAX_RETRIES = 5  # Max number of retries\n",
    "INITIAL_DELAY = 5  # Initial delay in seconds before retrying\n",
    "\n",
    "\n",
    "@retry(tries=MAX_RETRIES, delay=INITIAL_DELAY, backoff=2, jitter=(0, 1), exceptions=Exception)\n",
    "def process_text(row_dict: dict) -> List[GraphDocument]:\n",
    "    # Convert the row dictionary into a formatted string\n",
    "    text = \"\\n\".join([f\"{key}: {value}\" for key, value in row_dict.items()])\n",
    "    \n",
    "    # Create the Document object with the concatenated header-value string\n",
    "    doc = Document(page_content=text)\n",
    "    \n",
    "    # Process the document and return the result\n",
    "    return llm_transformer.convert_to_graph_documents([doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "MAX_WORKERS = 20\n",
    "NUM_ARTICLES = 20\n",
    "graph_documents = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submitting all tasks and creating a list of future objects\n",
    "    futures = [\n",
    "        executor.submit(process_text, row.to_dict())\n",
    "        for i, row in jobs.iterrows()\n",
    "    ]\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing documents\"):\n",
    "        try:\n",
    "            graph_document = future.result()\n",
    "            graph_documents.extend(graph_document)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph_documents from disk\n",
    "import pickle\n",
    "with open('graph_documents.pkl', 'rb') as f:\n",
    "    graph_documents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    baseEntityLabel=True,\n",
    "    include_source=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save graph_documents to disk to load them later\n",
    "import pickle\n",
    "# Save to a file using pickle\n",
    "with open('graph_documents.pkl', 'wb') as f:\n",
    "    pickle.dump(graph_documents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dist = graph.query(\n",
    "    \"\"\"\n",
    "MATCH (d:Document)\n",
    "RETURN d.text AS text,\n",
    "       count {(d)-[:MENTIONS]->()} AS entity_count\n",
    "\"\"\"\n",
    ")\n",
    "entity_dist_df = pd.DataFrame.from_records(entity_dist)\n",
    "entity_dist_df[\"token_count\"] = [\n",
    "    num_tokens_from_string(str(el)) for el in entity_dist_df[\"text\"]\n",
    "]\n",
    "# Scatter plot with regression line\n",
    "sns.lmplot(\n",
    "    x=\"token_count\", y=\"entity_count\", data=entity_dist_df, line_kws={\"color\": \"red\"}\n",
    ")\n",
    "plt.title(\"Entity Count vs Token Count Distribution\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Entity Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "degree_dist = graph.query(\n",
    "    \"\"\"\n",
    "MATCH (e:__Entity__)\n",
    "RETURN count {(e)-[:!MENTIONS]-()} AS node_degree\n",
    "\"\"\"\n",
    ")\n",
    "degree_dist_df = pd.DataFrame.from_records(degree_dist)\n",
    "\n",
    "# Calculate mean and median\n",
    "mean_degree = np.mean(degree_dist_df['node_degree'])\n",
    "percentiles = np.percentile(degree_dist_df['node_degree'], [25, 50, 75, 90])\n",
    "# Create a histogram with a logarithmic scale\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(degree_dist_df['node_degree'], bins=50, kde=False, color='blue')\n",
    "# Use a logarithmic scale for the x-axis\n",
    "plt.yscale('log')\n",
    "# Adding labels and title\n",
    "plt.xlabel('Node Degree')\n",
    "plt.ylabel('Count (log scale)')\n",
    "plt.title('Node Degree Distribution')\n",
    "# Add mean, median, and percentile lines\n",
    "plt.axvline(mean_degree, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean_degree:.2f}')\n",
    "plt.axvline(percentiles[0], color='purple', linestyle='dashed', linewidth=1, label=f'25th Percentile: {percentiles[0]:.2f}')\n",
    "plt.axvline(percentiles[1], color='orange', linestyle='dashed', linewidth=1, label=f'50th Percentile: {percentiles[1]:.2f}')\n",
    "plt.axvline(percentiles[2], color='yellow', linestyle='dashed', linewidth=1, label=f'75th Percentile: {percentiles[2]:.2f}')\n",
    "plt.axvline(percentiles[3], color='brown', linestyle='dashed', linewidth=1, label=f'90th Percentile: {percentiles[3]:.2f}')\n",
    "# Add legend\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\"\"\"\n",
    "MATCH (n:`__Entity__`)\n",
    "RETURN \"node\" AS type,\n",
    "       count(*) AS total_count,\n",
    "       count(n.description) AS non_null_descriptions\n",
    "UNION ALL\n",
    "MATCH (n)-[r:!MENTIONS]->()\n",
    "RETURN \"relationship\" AS type,\n",
    "       count(*) AS total_count,\n",
    "       count(r.description) AS non_null_descriptions\n",
    "\"\"\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vector = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(),\n",
    "    node_label='__Entity__',\n",
    "    text_node_properties=['id', 'description'],\n",
    "    embedding_node_property='embedding',\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "# project graph\n",
    "\n",
    "gds = GraphDataScience(\n",
    "    \"bolt://neo4j:7687\",\n",
    "    auth=(os.environ[\"NEO4J_USERNAME\"], os.environ[\"NEO4J_PASSWORD\"]),\n",
    "    database=\"neo4j\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G, result = gds.graph.project(\n",
    "    \"entities_new\",                   #  Graph name\n",
    "    \"__Entity__\",                 #  Node projection\n",
    "    \"*\",                          #  Relationship projection\n",
    "    nodeProperties=[\"embedding\"]  #  Configuration parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity_threshold = 0.95\n",
    "\n",
    "gds.knn.mutate(\n",
    "  G,\n",
    "  nodeProperties=['embedding'],\n",
    "  mutateRelationshipType= 'SIMILAR',\n",
    "  mutateProperty= 'score',\n",
    "  similarityCutoff=similarity_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.wcc.write(\n",
    "    G,\n",
    "    writeProperty=\"wcc\",\n",
    "    relationshipTypes=[\"SIMILAR\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_edit_distance = 3\n",
    "potential_duplicate_candidates = graph.query(\n",
    "    \"\"\"MATCH (e:`__Entity__`)\n",
    "    WHERE size(e.id) > 4 // longer than 4 characters\n",
    "    WITH e.wcc AS community, collect(e) AS nodes, count(*) AS count\n",
    "    WHERE count > 1\n",
    "    UNWIND nodes AS node\n",
    "    // Add text distance\n",
    "    WITH distinct\n",
    "      [n IN nodes WHERE apoc.text.distance(toLower(node.id), toLower(n.id)) < $distance | n.id] AS intermediate_results\n",
    "    WHERE size(intermediate_results) > 1\n",
    "    WITH collect(intermediate_results) AS results\n",
    "    // combine groups together if they share elements\n",
    "    UNWIND range(0, size(results)-1, 1) as index\n",
    "    WITH results, index, results[index] as result\n",
    "    WITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\n",
    "            CASE WHEN index <> index2 AND\n",
    "                size(apoc.coll.intersection(acc, results[index2])) > 0\n",
    "                THEN apoc.coll.union(acc, results[index2])\n",
    "                ELSE acc\n",
    "            END\n",
    "    )) as combinedResult\n",
    "    WITH distinct(combinedResult) as combinedResult\n",
    "    // extra filtering\n",
    "    WITH collect(combinedResult) as allCombinedResults\n",
    "    UNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\n",
    "    WITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\n",
    "    WHERE NOT any(x IN range(0,size(allCombinedResults)-1,1)\n",
    "        WHERE x <> combinedResultIndex\n",
    "        AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n",
    "    )\n",
    "    RETURN combinedResult\n",
    "    \"\"\", params={'distance': word_edit_distance})\n",
    "potential_duplicate_candidates[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = \"\"\"You are a data processing assistant. Your task is to identify duplicate entities in a list and decide which of them should be merged.\n",
    "The entities might be slightly different in format or content, but essentially refer to the same thing. Use your analytical skills to determine duplicates.\n",
    "\n",
    "Here are the rules for identifying duplicates:\n",
    "1. Entities with minor typographical differences should be considered duplicates.\n",
    "2. Entities with different formats but the same content should be considered duplicates.\n",
    "3. Entities that refer to the same real-world object or concept, even if described differently, should be considered duplicates.\n",
    "4. If it refers to different numbers, dates, or products, do not merge results\n",
    "\"\"\"\n",
    "user_template = \"\"\"\n",
    "Here is the list of entities to process:\n",
    "{entities}\n",
    "\n",
    "Please identify duplicates, merge them, and provide the merged list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from retry import retry\n",
    "\n",
    "class DuplicateEntities(BaseModel):\n",
    "    entities: List[str] = Field(\n",
    "        description=\"Entities that represent the same object or real-world entity and should be merged\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Disambiguate(BaseModel):\n",
    "    merge_entities: Optional[List[DuplicateEntities]] = Field(\n",
    "        description=\"Lists of entities that represent the same object or real-world entity and should be merged\"\n",
    "    )\n",
    "\n",
    "\n",
    "extraction_llm = ChatOpenAI(model_name=\"gpt-4o\").with_structured_output(\n",
    "    Disambiguate\n",
    ")\n",
    "\n",
    "extraction_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            user_template,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = extraction_prompt | extraction_llm\n",
    "\n",
    "@retry(tries=3, delay=2)\n",
    "def entity_resolution(entities: List[str]) -> Optional[List[str]]:\n",
    "    return [\n",
    "        el.entities\n",
    "        for el in extraction_chain.invoke({\"entities\": entities}).merge_entities\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_resolution(['Star Ocean The Second Story R', 'Star Ocean: The Second Story R'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_entities = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submitting all tasks and creating a list of future objects\n",
    "    futures = [\n",
    "        executor.submit(entity_resolution, el['combinedResult'])\n",
    "        for el in potential_duplicate_candidates\n",
    "    ]\n",
    "\n",
    "    for future in tqdm(\n",
    "        as_completed(futures), total=len(futures), desc=\"Processing documents\"\n",
    "    ):\n",
    "        to_merge = future.result()\n",
    "        if to_merge:\n",
    "            merged_entities.extend(to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_entities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\"\"\"\n",
    "UNWIND $data AS candidates\n",
    "CALL {\n",
    "  WITH candidates\n",
    "  MATCH (e:__Entity__) WHERE e.id IN candidates\n",
    "  RETURN collect(e) AS nodes\n",
    "}\n",
    "CALL apoc.refactor.mergeNodes(nodes, {properties: {\n",
    "    `.*`: 'discard'\n",
    "}})\n",
    "YIELD node\n",
    "RETURN count(*)\n",
    "\"\"\", params={\"data\": merged_entities})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
